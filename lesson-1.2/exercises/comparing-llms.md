# Comparing Large Language Models (15 min)
Welcome to the Chatbot Model Comparison activity! Here, you will compare the output of three different chatbot models. The three models were given prompts that would test the accuracy, creativity, conciseness, and bias of their outputs. Your job is to select the model that performed best in each category. Let's get started!

Complete the activity [here](https://igfnaqfcyl-13589482-i.codehs.me/index.html).  Then edit this page and write down your reflections here:

### Which model did you find performed best overall, and why?
Llama 3 performed the best with these prompts.

### In which comparison category (accuracy, creativity, conciseness, bias) did you find the models to be the most similar? What about the most different?
They all seemed to be equally accurate in their responses, most of the prompts had a similiar structure and ifnromation and only differed in how much information was offered rather than the accuracy of the information.

### Were you surprised by any of the results?
I was suprised that the AI model that seemed to be the most creative, accurate and least biased was an AI model I dont use much and I think it shows how niche AI's like Llama 3 who are trained differently can vastly change the responses to prompts we get while large AI models like ChatGPT and Gemini and more exposed to large databases which could limit how creative its responses are.

### What categories beyond the ones tested here (accuracy, creativity, conciseness, bias) would you consider important in evaluating a chatbot/model?

I think context is important, can the AI create a response that relates to the user for example, example B asks for a new word that means a sense of missing a place you visited. I think the AI should be able to take that prompt, understand where the user is coming from and create a word that holds meaning to the user rather than just adding phrases to "nostolgia" or making up fantasy words rather than meaningful ones.
